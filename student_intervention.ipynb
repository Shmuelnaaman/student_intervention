{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "The goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. :\n",
    "In this case we are solving a “classification problem”. Each student in the data set will be categorize according to the parameters as \"need early intervention\" or does not \"need early intervention\". The algorithm will fit each student into one of these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"F:\\\\Nanodegree\\\\student_intervention\\\\student-data.csv\")\n",
    "print (\"Student data read successfully!\")\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistics facts about the dataset?\n",
    "- Total number of students : 395\n",
    "- Number of students who passed : 265\n",
    "- Number of students who failed : 130\n",
    "- Graduation rate of the class (%) :  67%\n",
    "- Number of features : 31\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 31\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# Compute desired values \n",
    "n_students = len(student_data)\n",
    "n_features = len(student_data.columns)\n",
    "n_passed = len(student_data[student_data['passed']=='yes'])\n",
    "n_failed = len(student_data[student_data['passed']=='no'])\n",
    "grad_rate =100* n_passed/n_students   \n",
    "\n",
    "print (\"Total number of students: {}\".format(n_students))\n",
    "print (\"Number of students who passed: {}\".format(n_passed))\n",
    "print (\"Number of students who failed: {}\".format(n_failed))\n",
    "print (\"Number of features: {}\".format(n_features))\n",
    "print (\"Graduation rate of the class: {:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "\n",
    "Separate the data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
       "\n",
       "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
       "0   ...       yes       no        no       4         3     4    1    1      3   \n",
       "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
       "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
       "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
       "4   ...       yes       no        no       4         3     2    1    2      5   \n",
       "\n",
       "  absences  \n",
       "0        6  \n",
       "1        4  \n",
       "2       10  \n",
       "3        2  \n",
       "4        4  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print (\"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print (\"Target column: {}\".format(target_col))\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print (\"\\nFeature values:-\")\n",
    "X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As we can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  \n",
    "            # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print (\"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, \n",
    "                                                                     y_all, \n",
    "                                                                     test_size=0.24, \n",
    "                                                                     random_state=0)\n",
    "\n",
    "print (\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Support Vector Machines Classification O(n3): \n",
    " ### General applications: \n",
    "Given, data points that belong into two clusters, each data point can be represented by a n-dimensional vector. It is possible to find many n-1 dimensional hyper planes that will separate the data points into two clusters. Support Vector Machines Classification will find the hyper plane that represents the hyper plane with maximum separation or margins. SVM is helpful in text categorization, images classifications, highly efficient in protein classifications and classification of hand writers’ characters. \n",
    " ### Strengths: \n",
    "Several specialized algorithms that enable quick solving of the quadratic programming problem arise from the model. Kernel SVM is available in many toolkits. For images classifications SVMs achieve significantly higher search accuracy than traditional query refinement schemes. \n",
    " ### Weaknesses: \n",
    "The model is not easy to interpret. The classification is into two class and the solution is uncelebrated (we cannot estimate the degree of certainty of a given solutions). All input data must be labeled (no unsupervised learning). \n",
    " ### Why did you choose this model to apply? \n",
    "The structure of the data set is a classic case for SVM, each label can be describe with a vector of values. The problem that we want to solve is classification. I hope that the training will be done in a short time. The solution that I need is binary and I do not care about the certainty for each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.011\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print (\"Training {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nTraining time (secs): {:.3f}\".format(end - start))\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(C=2)\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.011\n",
      "F1 score for training set: 0.9135254988913526\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.006\n",
      "F1 score for test set: 0.7464788732394366\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.9343065693430657\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.7801418439716312\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.008\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.005\n",
      "F1 score for training set: 0.910958904109589\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.7746478873239436\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.016\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.012\n",
      "F1 score for training set: 0.9135254988913526\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.004\n",
      "F1 score for test set: 0.7464788732394366\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print (\"------------------------------------------\")\n",
    "    print (\"Training set size: {}\".format(len(X_train)))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print (\"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Adaptive Boosting Classification\n",
    " ###  General applications: \n",
    "A Meta-algorithm that can be used in conjunction with other types of learning algorithms and improve their performance. The output of the other learning algorithms ('weak learners') is combined as a weighted sum to create the boosted classifier. AdaBoost used as a Standard algorithm for Face and object Detection, \n",
    " ### Strengths: \n",
    "Less susceptible to overfitting than other learning algorithms, the best out-of-the-box classifier. \n",
    " ### Weaknesses: \n",
    "Sensitive to uniform noisy data and outliers, AdaBoost depends on data and weak learner and can fail if weak classifiers are overfit or underfit.\n",
    " ### Why did you choose this model to apply?  \n",
    "As part of the exploration I wanted to test a predictive model that will be less susceptible to overfitting and since Adaboost is consider the best out-of-the-box classifier, I wanted to check if a boosting model will do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.005\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=2)\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 0.8278867102396513\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.7887323943661971\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.007\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 0.8169014084507042\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.7887323943661971\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.005\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.8243243243243242\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.7971014492753624\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.005\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.8278867102396513\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.7887323943661971\n"
     ]
    }
   ],
   "source": [
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees\n",
    " ### General applications:  \n",
    "Ensemble learning method for classification, that construct a multitude decision trees at training, outputting the class of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm, inducing random forest \"bagging\" idea and the random selection of features, in order to construct a collection of decision trees with controlled variance. Random forests use tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. Extremely randomized trees or ExtraTrees are trained like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. \n",
    " ### Strengths: \n",
    "More complex classifier (a larger forest) getting more accurate nearly monotonically. \n",
    " ### Weaknesses: \n",
    "For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n",
    "### Why did you choose this model to apply?\n",
    "The fact that we deal with students and as we know humans are not easy to predict, a model that will include randomization in the process of creating classifiers might captured something that could not be captured by the well formulated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.093\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=40)\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.019\n",
      "F1 score for training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.013\n",
      "F1 score for test set: 0.7913669064748202\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.086\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.007\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.007\n",
      "F1 score for test set: 0.7659574468085107\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.068\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.008\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.008\n",
      "F1 score for test set: 0.75\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.077\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.009\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.007\n",
      "F1 score for test set: 0.7591240875912408\n"
     ]
    }
   ],
   "source": [
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "\n",
    "The training time of SVM achieve the shortest time with 0.01 sec, where the Adaboost and the Extremely Randomized Trees achieved 0.06 0.1 respectively. This is expected, considering the fact that SVM is usually fast model.\n",
    "\n",
    "Prediction in all cases is ~ 10 times lower than the corresponding training time (SVM 0.003 Adaboost 0.005 Extremely Randomized Trees 0.010).\n",
    "\n",
    "F1 score for the SVM model depict decreasing trend as Training set size increase, where F1 score for the Adaboost and Extremely Randomized Trees depict increasing trend.\n",
    "\n",
    "Considering the above, and the given problem limitations, the most appropriate algorithm is the Adaboost. The main reason for that is the nature of the data set, which is expected to be large and even larger in the feature. SVM is the fastest model, but the performance of the mode decrease for larger training size. Adaboost provides a stable or even increasing performance as the training set increase and reasonable computation time that is longer than the SVM but shorter than the Extremely Randomized Trees.\n",
    "\n",
    "\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "\n",
    "AdaBoost uses a number of training sample to pick a number of good 'classifiers'. AdaBoost will look at a number of classifiers and find out which is the best predictor of a label based on the sample. After it has chosen the best classifier it will continue to find another until some threshold is reached and those classifiers combined together will provide the end result.\n",
    "\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?\n",
    "F1 score for test set: 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#Grid search\n",
    "regressor = AdaBoostClassifier()\n",
    "parameters = {'n_estimators':(2, 4, 8, 20, 40, 80, 150)}\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "    return f1_score(label, prediction, pos_label='yes')\n",
    "\n",
    "scorer = make_scorer(performance_metric, greater_is_better=True)\n",
    "\n",
    "reg = GridSearchCV(regressor, parameters, scorer, cv=5)\n",
    "reg.fit(X_all, y_all)\n",
    "clf = reg.best_estimator_\n",
    "\n",
    "print (clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.007\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 0.8278867102396513\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.7887323943661971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference\n",
    "\n",
    "http://wikipedia.org/ \n",
    "\n",
    "http://stackoverflow.com/\n",
    "\n",
    "http://scikit-learn.org/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
