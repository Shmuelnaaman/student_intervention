{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "The goal is to identify students who might need early intervention - In this case we are solving a “classification problem”. Each student in the data set will be categorize according to the parameters as \"need early intervention\" or does not \"need early intervention\". The algorithm will fit each student into one of these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print (\"Student data read successfully!\")\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some statistics facts about the dataset\n",
      " - Total number of students: 395\n",
      " - Number of students who passed: 265\n",
      " - Number of students who failed: 130\n",
      " - Number of features: 30\n",
      " - Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# Compute desired values \n",
    "n_students = len(student_data)\n",
    "n_features = len(student_data.columns)-1\n",
    "n_passed   = len(student_data[student_data['passed']=='yes'])\n",
    "n_failed   = len(student_data[student_data['passed']=='no'])\n",
    "grad_rate  = 100*n_passed/float(n_students)\n",
    "\n",
    "print (\"Some statistics facts about the dataset\")\n",
    "\n",
    "print (\" - Total number of students: {}\".format(n_students))\n",
    "print (\" - Number of students who passed: {}\".format(n_passed))\n",
    "print (\" - Number of students who failed: {}\".format(n_failed))\n",
    "print (\" - Number of features: {}\".format(n_features))\n",
    "print (\" - Graduation rate of the class: {:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "\n",
    "Separate the data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
       "\n",
       "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
       "0   ...       yes       no        no       4         3     4    1    1      3   \n",
       "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
       "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
       "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
       "4   ...       yes       no        no       4         3     2    1    2      5   \n",
       "\n",
       "  absences  \n",
       "0        6  \n",
       "1        4  \n",
       "2       10  \n",
       "3        2  \n",
       "4        4  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print (\"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print (\"Target column: {}\".format(target_col))\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print (\"\\nFeature values:-\")\n",
    "X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As we can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  \n",
    "            # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print (\"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. Next, we split the data (both features and corresponding labels) into training and test sets. In addition I set the random state of the train_test_split function so the results will be reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# Select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, \n",
    "                                                                     y_all, \n",
    "                                                                     test_size = 0.24, random_state = 42)\n",
    "\n",
    "print (\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Next I choose three supervised learning models that are available in scikit-learn, and appropriate for this problem.\n",
    "\n",
    "For each model : \n",
    "- Discussion about the general applications of the model. The strength and weaknesses of the model.\n",
    "- Reasons for choosing this model.\n",
    "\n",
    "- Fit the model to the training data, predict labels (for both training and test sets), and measure the F<sub>1</sub> score with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Support Vector Machines Classification O(n3): \n",
    " ### General applications: \n",
    "Given, data points that belong into two clusters, each data point can be represented by a n-dimensional vector. It is possible to find many n-1 dimensional hyper planes that will separate the data points into two clusters. Support Vector Machines Classification will find the hyper plane with maximum separation or margins. \n",
    "\n",
    "### general applications \n",
    "SVM is helpful in text categorization, images classifications, highly efficient in protein classifications and classification of hand writers’ characters.\n",
    "### The advantages of support vector machines are:\n",
    " - Effective in high dimensional spaces.\n",
    " - Still effective in cases where number of dimensions is greater than the number of samples.\n",
    " - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    " - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also \n",
    " - possible to specify custom kernels.\n",
    " \n",
    "### The disadvantages of support vector machines include:\n",
    " - If the number of features is much greater than the number of samples, the method is likely to give poor performances.\n",
    " - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    " - Several specialized algorithms enable quick solution of the quadratic programming problem. Kernel SVM is available in many toolkits. SVM is able to separate feature spaces that are not linearly separated. SVM is effective for problem with high dimensional space. SVM is effective for cases where number of dimensions is greater than the number of samples.  SVM is Memory efficient. [ http://scikit-learn.org/stable/modules/svm.html ]\n",
    " - Parameters of a solved model are difficult to interpret [ https://en.wikipedia.org/wiki/Support_vector_machine ]. The classification is into two class and the solution is un-celebrated (we cannot estimate the degree of certainty of a given solutions). All input data must be labeled (no unsupervised learning). If the number of features is much greater than the number of samples, the method is likely to give poor performances\n",
    " \n",
    "### Reasons for choosing this model. \n",
    "The structure of the data set is a classic case for SVM, each label can be describe with a vector of values. The problem that we want to solve is classification. The fact that the data set is small will make sure that the training will be done in a short time. The data set might not be high dimensional but consider the small size of the data set, seems as SVM will have an advantage with the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.023\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print (\"Training {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nTraining time (secs): {:.3f}\".format(end - start))\n",
    "\n",
    "# Choose a model, import it and instantiate an object\n",
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.013\n",
      "F1 score for training set: 0.876068376068\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.005\n",
      "F1 score for test set: 0.783783783784\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.004\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 0.877697841727\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.774647887324\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.013\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.006\n",
      "F1 score for training set: 0.867924528302\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.781456953642\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.016\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.011\n",
      "F1 score for training set: 0.876068376068\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.004\n",
      "F1 score for test set: 0.783783783784\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print (\"------------------------------------------\")\n",
    "    print (\"Training set size: {}\".format(len(X_train)))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print (\"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Adaptive Boosting Classification\n",
    " ###  General applications: \n",
    "A Meta-algorithm that can be used in conjunction with other types of learning algorithms and improve their performance. The output of the other learning algorithms ('weak learners') is combined as a weighted sum to create the boosted classifier. AdaBoost used as a Standard algorithm for Face and object Detection,\n",
    "### Strengths: \n",
    "AdaBoost is less susceptible to over fitting than other learning algorithms. AdaBoost is the best out-of-the-box classifier. AdaBoost is simple to implement. AdaBoost is can perform feature selection\n",
    "### Weaknesses: \n",
    "AdaBoost is sensitive to uniform noisy data and outliers. AdaBoost depends on data and weak learner and can fail if weak classifiers are overfit or underfit.\n",
    "### Reasons for choosing this model.\n",
    "As part of the exploration I wanted to test a predictive model that will be less susceptible to overfitting and since Adaboost is consider the best out-of-the-box classifier, I wanted to check if a boosting model will do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.356\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "# Choose a model, import it and instantiate an object\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(random_state = 42 )\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.023\n",
      "F1 score for training set: 0.863741339492\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.021\n",
      "F1 score for test set: 0.781954887218\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.271\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.016\n",
      "F1 score for training set: 0.948148148148\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.019\n",
      "F1 score for test set: 0.766917293233\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.340\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.021\n",
      "F1 score for training set: 0.892733564014\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.015\n",
      "F1 score for test set: 0.828125\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.330\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.021\n",
      "F1 score for training set: 0.863741339492\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.017\n",
      "F1 score for test set: 0.781954887218\n"
     ]
    }
   ],
   "source": [
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees\n",
    " ### General applications:  \n",
    "Ensemble learning method for classification, that construct a multitude decision trees at training, outputting the class of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm, inducing random forest \"bagging\" idea and the random selection of features, in order to construct a collection of decision trees with controlled variance. Random forests use tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. Extremely randomized trees or ExtraTrees are trained like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. Hydrological problem. Computational intensive problems. Modelling of large datasets and input selection.\n",
    "### Strengths: \n",
    "More complex classifier (a larger forest) getting more accurate nearly monotonically.  Combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. effective option to balance accuracy and computational efficiency in data-driven modelling.[ http://www.hydrol-earth-syst-sci.net/17/2669/2013/hess-17-2669-2013.pdf ]\n",
    "### Weaknesses: \n",
    "The larger the  number of trees in the forest the  better, but also the longer it will take to compute. The results will stop getting significantly better beyond a critical number of trees.\n",
    "### Reasons for choosing this model.\n",
    "The fact that we deal with students and as we know humans are not easy to predict, a model that will include randomization in the process of creating classifiers might captured something that could not be captured by the well formulated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.091\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "# Choose a model, import it and instantiate an object\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.005\n",
      "F1 score for training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.766917293233\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.070\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.004\n",
      "F1 score for test set: 0.736842105263\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.062\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.751879699248\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training ExtraTreesClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.065\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using ExtraTreesClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.766917293233\n"
     ]
    }
   ],
   "source": [
    "train_predict(clf, X_train[0:100], y_train[0:100], X_test, y_test)\n",
    "train_predict(clf, X_train[0:200], y_train[0:200], X_test, y_test)\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>SVC</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.016 | 0.004 | 0.876 | 0.783 \n",
    "| Training 200 | 0.008 | 0.002 | 0.867 | 0.781 \n",
    "| Training 100 | 0.003 | 0.002 | 0.877 | 0.774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>AdaBoostClassifier</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.200 | 0.012 | 0.863 | 0.781 \n",
    "| Training 200 | 0.197 | 0.011 | 0.892 | 0.828 \n",
    "| Training 100 | 0.202 | 0.016 | 0.948 | 0.766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "<header>\n",
    "   <h3>ExtraTreesClassifier</h3>\n",
    "</header>\n",
    "</section>\n",
    "\n",
    "| Dataset | Training Time | Prediction Time | Training F1 Score | Test F1 Score         \n",
    "| :-: | :-: |:-: | :-: | :-: \n",
    "| Training 300 | 0.049 | 0.002 | 1.0 | 0.766 \n",
    "| Training 200 | 0.049 | 0.002 | 1.0 | 0.751 \n",
    "| Training 100 | 0.046 | 0.002 | 1.0 | 0.736"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments and the results, a short explaintion about the optimal model of choice. The comparison will consider  the available data, limited resources, cost, and performance?\n",
    "\n",
    "The training time for  SVM  is  0.01 sec, Adaboost 0.2 and for Extremely Randomized Trees 0.05.\n",
    "Prediction time for  SVM  is  0.003 sec, Adaboost 0.01 and for Extremely Randomized Trees 0.002.\n",
    "F1 score for the SVM is 0.78, Adaboost 0.79, and for Extremely Randomized Trees 0.75. (Running the model with different random state provides different results, but the  difference is not significant). \n",
    "\n",
    "Considering the above, and the given problem limitations, the most appropriate algorithm is the Adaboost. Adaboost performed best when considering the Prediction time and the performance. The long training time is not significant if we consider the fact that the problem is not real time and the data set is quite small. The school will definitely will prefer more accurate model than faster model.\n",
    "\n",
    "- A short explenation in layman's terms how the AdaBoost is trained and predict.\n",
    "\n",
    "AdaBoost uses a number of training samples to pick a number of good 'classifiers'. In this case we consider classifiers as the features that describe each student. Good features (classifiers) will be features that distinguish between students that need intervention to  students that do not need interventions.  AdaBoost will look at a number of classifiers and find out which is the best predictor of a label (intervention) based on the sample. After it has chosen the best classifier it will continue to find another classifier until some threshold is reached and those classifiers combined together will provide the end result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-tune the model. Use Gridsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Grid search\n",
    "regressor = AdaBoostClassifier( DecisionTreeClassifier(class_weight=None, \n",
    "                                                       criterion='gini', \n",
    "                                                       max_depth=3,\n",
    "                                                       max_features=None, \n",
    "                                                       max_leaf_nodes=None, \n",
    "                                                       min_samples_leaf=2,\n",
    "                                                       min_samples_split=2, \n",
    "                                                       min_weight_fraction_leaf=0.1,\n",
    "                                                       presort=False,\n",
    "                                                       random_state=42,\n",
    "                                                       splitter='best'),\n",
    "                               random_state = 42)\n",
    "\n",
    "parameters = {'n_estimators':( 115,110), 'learning_rate' :(0.006, 0.005), 'algorithm': ['SAMME.R']}\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "    return f1_score(label, prediction, pos_label='yes')\n",
    "\n",
    "scorer = make_scorer(performance_metric, greater_is_better=True)\n",
    "\n",
    "reg = GridSearchCV(regressor, parameters, scorer, cv=5)\n",
    "reg.fit(X_train, y_train)\n",
    "clf = reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.1,\n",
      "            presort=False, random_state=42, splitter='best'),\n",
      "          learning_rate=0.005, n_estimators=115, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print (clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.781\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.050\n",
      "F1 score for training set: 0.833333333333\n",
      "Predicting labels using AdaBoostClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.029\n",
      "F1 score for test set: 0.802816901408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_predict(clf, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the model's final F<sub>1</sub> score?\n",
    "F1 score for test set: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference\n",
    "\n",
    "http://wikipedia.org/ \n",
    "\n",
    "http://stackoverflow.com/\n",
    "\n",
    "http://scikit-learn.org/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
